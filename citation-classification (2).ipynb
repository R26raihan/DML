{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":82370,"databundleVersionId":12656064,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pymupdf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T02:29:10.244035Z","iopub.execute_input":"2025-07-07T02:29:10.244552Z","iopub.status.idle":"2025-07-07T02:29:16.501722Z","shell.execute_reply.started":"2025-07-07T02:29:10.244529Z","shell.execute_reply":"2025-07-07T02:29:16.500966Z"}},"outputs":[{"name":"stdout","text":"Collecting pymupdf\n  Downloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\nDownloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pymupdf\nSuccessfully installed pymupdf-1.26.3\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport re\nfrom pathlib import Path\nfrom typing import List, Dict\nimport fitz  # PyMuPDF\nimport pandas as pd\nimport torch\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom sklearn.model_selection import train_test_split, KFold\nfrom lxml import etree\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# ==============================================================================\n# 1. KONFIGURASI DAN KONSTANTA\n# ==============================================================================\nINPUT_DIR = Path(\"/kaggle/input/make-data-count-finding-data-references\")\nPDF_TRAIN_DIR = INPUT_DIR / \"train/PDF\"\nXML_TRAIN_DIR = INPUT_DIR / \"train/XML\"\nTRAIN_LABELS_PATH = INPUT_DIR / \"train_labels.csv\"\nPDF_TEST_DIR = INPUT_DIR / \"test/PDF\"\nXML_TEST_DIR = INPUT_DIR / \"test/XML\"\n\nMODEL_NAME = \"Qwen/Qwen2-7B-Instruct\"  # Ganti ke Qwen2.5-32B-Instruct-AWQ jika tersedia\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nBATCH_SIZE = 4\nVALIDATION_SET_SIZE = 0.2\nRANDOM_STATE = 42\nN_FOLDS = 5\n\n# Valid dataset DOI prefixes\nVALID_DATASET_PREFIXES = [\"10.5061/dryad\", \"10.5281/zenodo\", \"10.25386/genetics\", \"10.7937\"]\n\n# ==============================================================================\n# 2. REGULAR EXPRESSIONS (REGEX)\n# ==============================================================================\nRE_DOI = re.compile(r\"\\b(10\\.\\d{4,9}/[-._;()/:A-Z0-9]+)\", re.IGNORECASE)\nACCESSION_PATTERNS = [\n    \"GSE\\d+\", \"SR[APRX]\\d+\", \"PRJ[NAED][A-Z]?\\d+\", \"EPI(?:_ISL_)?\\d+\",\n    \"PXD\\d{6}\", \"SAM[ND]\\d+\", \"ERR\\d+\", \"PDB\\s+[A-Z0-9]+\", \"E-MTAB-\\d+\",\n    \"IPR\\d{6}\", \"PF\\d{5}\", \"EMPIAR-\\d{5}\", \"CHEMBL\\d+\", \"CVCL_[A-Z0-9]{4}\",\n    \"ENS[A-Z]{0,6}[GT]\\d{11}\", \"N[MC]_\\d+(?:\\.\\d+)?\", \"rs\\d+\",\n    \"uniprot:(?:[OPQ][0-9][A-Z0-9]{3}[0-9]|[A-NR-Z][0-9][A-Z][A-Z0-9]{2}[0-9])\",\n]\nRE_ACCESSION = re.compile(r\"\\b(\" + \"|\".join(ACCESSION_PATTERNS) + r\")\\b\", re.IGNORECASE)\nRE_REFERENCES_SECTION = re.compile(\n    r\"^(REFERENCES?|BIBLIOGRAPHY|Literature\\s+Cited|Works\\s+Cited|\\d+\\.?\\s+(REFERENCES?|Bibliography))(:)?$\",\n    re.IGNORECASE | re.MULTILINE\n)\nRE_CITATION_PATTERNS = [\n    r'\\(\\d{4}\\)',  # (2020)\n    r'\\d{4}\\.',     # 2020.\n    r'doi:',        # doi:\n    r'\\bet al\\b',   # et al\n]\nRE_CLEAN_LLM_OUTPUT = re.compile(r\"^\\s*([ABC])\\b\", re.MULTILINE)\n\n# ==============================================================================\n# 3. FUNGSI EKSTRAKSI DATA\n# ==============================================================================\ndef normalize_doi(doi: str) -> str:\n    doi = doi.strip().lower()\n    if not doi.startswith(\"https://doi.org/\"):\n        doi = doi.lstrip(\"doi:\").strip()\n        if doi.startswith(\"10.\"):\n            return f\"https://doi.org/{doi}\"\n    return doi\n\ndef is_valid_dataset_doi(doi: str) -> bool:\n    return any(doi.lower().startswith(prefix) for prefix in VALID_DATASET_PREFIXES)\n\ndef remove_references_section(text: str) -> str:\n    lines = text.split('\\n')\n    cut_index = -1\n    for i in range(len(lines) - 1, max(0, int(len(lines) * 0.3)), -1):\n        line = lines[i].strip()\n        if RE_REFERENCES_SECTION.match(line):\n            following_lines = lines[i+1:i+4]\n            has_citations = False\n            for follow_line in following_lines:\n                if follow_line.strip() and any(re.search(pat, follow_line, re.IGNORECASE) for pat in RE_CITATION_PATTERNS):\n                    has_citations = True\n                    break\n            if has_citations or i >= len(lines) - 3:\n                cut_index = i\n                break\n    if cut_index != -1:\n        ref_section = '\\n'.join(lines[cut_index:])\n        if RE_DOI.search(ref_section) or RE_ACCESSION.search(ref_section):\n            return text\n        return '\\n'.join(lines[:cut_index]).strip()\n    return text.strip()\n\ndef extract_text_from_xml(xml_path: Path) -> str:\n    try:\n        tree = etree.parse(str(xml_path))\n        sections = tree.xpath(\"//sec[@sec-type='materials|methods' or @sec-type='data' or @sec-type='supplementary-material']//p/text()\")\n        return \"\\n\".join(sections) if sections else \"\"\n    except Exception:\n        return \"\"\n\ndef find_potential_citations(text: str, article_id: str) -> List[Dict[str, str]]:\n    citations = []\n    paragraphs = text.split(\"\\n\\n\")\n    for para in paragraphs:\n        para = para.strip()\n        if not para:\n            continue\n        patterns = {\"doi\": RE_DOI, \"accession\": RE_ACCESSION}\n        for source, pattern in patterns.items():\n            for match in pattern.finditer(para):\n                dataset_id = match.group(0)\n                if source == \"doi\":\n                    dataset_id = normalize_doi(dataset_id)\n                    if not is_valid_dataset_doi(dataset_id) or dataset_id == f\"https://doi.org/{article_id}\":\n                        continue\n                citations.append({\n                    \"article_id\": article_id,\n                    \"text\": para,\n                    \"dataset_id\": dataset_id,\n                    \"source\": source\n                })\n    return citations\n\ndef extract_chunks_from_paths(pdf_paths: List[Path], xml_dir: Path) -> pd.DataFrame:\n    all_chunks = []\n    print(f\"Memulai ekstraksi dari {len(pdf_paths)} file...\")\n    for pdf_path in tqdm(pdf_paths, desc=\"üìÑ Mengekstrak File\"):\n        article_id = pdf_path.stem\n        try:\n            xml_path = xml_dir / f\"{article_id}.xml\"\n            if xml_path.exists():\n                full_text = extract_text_from_xml(xml_path)\n            else:\n                with fitz.open(pdf_path) as doc:\n                    full_text = \"\\n\".join([page.get_text(\"text\") for page in doc])\n            cleaned_text = remove_references_section(full_text)\n            chunks = find_potential_citations(cleaned_text, article_id)\n            all_chunks.extend(chunks)\n        except Exception as e:\n            print(f\"‚ö†Ô∏è Gagal memproses file {article_id}: {e}\")\n    return pd.DataFrame(all_chunks)\n\n# ==============================================================================\n# 4. FUNGSI KLASIFIKASI LLM\n# ==============================================================================\ndef build_prompt_messages(batch_df: pd.DataFrame) -> List[Dict[str, str]]:\n    system_message = \"\"\"You are a meticulous research analyst specializing in data citation. Your task is to analyze text snippets from scientific papers and classify the relationship between the paper and a mentioned dataset ID.\n\nFollow these rules for classification:\n- **Primary (A)**: The data was **generated by the authors for this specific study**. Look for phrases like \"data are available\", \"we generated\", \"our data have been deposited in\", \"supplemental material\", \"data for this study\", \"deposited at\", \"available at\".\n- **Secondary (B)**: The data was **reused from an external source** or a previous study. Look for phrases like \"data were obtained from\", \"retrieved from\", \"we used the dataset from\", \"downloaded from\", \"sourced from\", \"accessed from\".\n- **None (C)**: The ID refers to something else (e.g., another publication), is mentioned in passing, or there is insufficient context.\n\nExamples:\n1. ID: https://doi.org/10.5061/dryad.6m3n9\n   Context: \"The data we used in this publication can be accessed from Dryad at doi:10.5061/dryad.6m3n9.\"\n   Classification: A\n2. ID: pdb 5yfp\n   Context: \"Structure of the Saccharomyces cerevisiae exocyst holomeric octameric complex... (pdb 5yfp).\"\n   Classification: B\n3. ID: https://doi.org/10.1098/rspb.2016.1151\n   Context: \"As described in a previous study (doi:10.1098/rspb.2016.1151).\"\n   Classification: C\n4. ID: GSE37569\n   Context: \"Primary data for Agilent and Affymetrix microarray experiments are available at the NCBI Gene Expression Omnibus (GEO, http://www.ncbi.nlm.nih.gov/geo/) under the accession numbers GSE37569.\"\n   Classification: A\n5. ID: E-MTAB-10217\n   Context: \"The datasets presented in this study can be found in online repositories... E-MTAB-10217.\"\n   Classification: A\n\nFor each item below:\n1. Read the Context carefully.\n2. Analyze the language used (generated or reused).\n3. Decide the final classification: A, B, or C.\n\nOutput ONLY a list of single letters (A, B, or C), each on a new line.\n\"\"\"\n    user_prompts = []\n    for i, row in enumerate(batch_df.itertuples(), 1):\n        snippet = ' '.join(str(row.text).split())\n        user_prompts.append(f\"{i}. ID: {row.dataset_id}\\n   Context: \\\"...{snippet}...\\\"\")\n    messages = [\n        {\"role\": \"system\", \"content\": system_message},\n        {\"role\": \"user\", \"content\": \"\\n\\n\".join(user_prompts)}\n    ]\n    return messages\n\ndef classify_batch_with_llm(batch_df: pd.DataFrame, model, tokenizer) -> List[str]:\n    messages = build_prompt_messages(batch_df)\n    prompt_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    inputs = tokenizer(prompt_text, return_tensors=\"pt\", truncation=True, max_length=4096).to(model.device)\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=100,\n        do_sample=True,\n        temperature=0.4,\n        top_p=0.9,\n        pad_token_id=tokenizer.eos_token_id\n    )\n    decoded_output = tokenizer.decode(outputs[0, inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n    labels = RE_CLEAN_LLM_OUTPUT.findall(decoded_output.strip())\n    return labels + [\"C\"] * (len(batch_df) - len(labels))\n\ndef run_llm_classification(df_chunks: pd.DataFrame, model, tokenizer):\n    results = []\n    print(f\"\\nMemulai klasifikasi dengan LLM untuk {len(df_chunks)} chunk...\")\n    for i in tqdm(range(0, len(df_chunks), BATCH_SIZE), desc=\"ü§ñ Mengklasifikasi\"):\n        batch_df = df_chunks.iloc[i:i+BATCH_SIZE].reset_index(drop=True)\n        try:\n            labels = classify_batch_with_llm(batch_df, model, tokenizer)\n            for j, label_code in enumerate(labels):\n                if label_code in [\"A\", \"B\"]:\n                    row = batch_df.iloc[j]\n                    results.append({\n                        \"article_id\": row.article_id,\n                        \"dataset_id\": row.dataset_id,\n                        \"type\": \"Primary\" if label_code == \"A\" else \"Secondary\"\n                    })\n        except Exception as e:\n            print(f\"‚ùå Error pada batch {i//BATCH_SIZE}: {e}\")\n            continue\n    df_results = pd.DataFrame(results)\n    return df_results.drop_duplicates(subset=['article_id', 'dataset_id']).reset_index(drop=True)\n\n# ==============================================================================\n# 5. FUNGSI EVALUASI\n# ==============================================================================\ndef calculate_f1_score(true_labels: pd.DataFrame, pred_labels: pd.DataFrame):\n    print(\"\\n--- HASIL EVALUASI ---\")\n    if pred_labels.empty:\n        print(\"Tidak ada prediksi valid (A/B) yang dihasilkan. F1-Score adalah 0.\")\n        return 0\n    true_set = set(map(tuple, true_labels[['article_id', 'dataset_id', 'type']].astype(str).apply(lambda x: x.str.lower()).values))\n    pred_set = set(map(tuple, pred_labels[['article_id', 'dataset_id', 'type']].astype(str).apply(lambda x: x.str.lower()).values))\n    tp = len(true_set.intersection(pred_set))\n    fp = len(pred_set - true_set)\n    fn = len(true_set - pred_set)\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n    print(f\"üìä True Positives (TP) : {tp}\")\n    print(f\"üìä False Positives (FP): {fp}\")\n    print(f\"üìä False Negatives (FN): {fn}\")\n    print(\"-\" * 25)\n    print(f\"üéØ Precision: {precision:.4f}\")\n    print(f\"üîç Recall   : {recall:.4f}\")\n    print(f\"‚≠ê F1-Score : {f1:.4f}\")\n    print(\"-\" * 25)\n    return f1\n\n# ==============================================================================\n# 6. BLOK EKSEKUSI UTAMA (MAIN)\n# ==============================================================================\nif __name__ == \"__main__\":\n    # --- Langkah 1: Muat data pelatihan ---\n    print(\"Membaca data pelatihan...\")\n    df_labels_all = pd.read_csv(TRAIN_LABELS_PATH)\n    all_article_ids = df_labels_all['article_id'].unique()\n    \n    # --- Langkah 2: Cross-validation ---\n    print(f\"\\nMelakukan {N_FOLDS}-fold cross-validation...\")\n    kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n    f1_scores = []\n    \n    for fold, (train_idx, val_idx) in enumerate(kf.split(all_article_ids)):\n        print(f\"\\nFold {fold + 1}/{N_FOLDS}\")\n        train_ids = all_article_ids[train_idx]\n        val_ids = all_article_ids[val_idx]\n        true_val_labels = df_labels_all[df_labels_all['article_id'].isin(val_ids)].copy()\n        val_pdf_paths = [p for p in PDF_TRAIN_DIR.glob(\"*.pdf\") if p.stem in val_ids]\n        print(f\"‚úÖ Data dibagi: {len(train_ids)} untuk latihan, {len(val_ids)} untuk validasi.\")\n        \n        # --- Langkah 3: Ekstrak chunks untuk validasi ---\n        df_val_chunks = extract_chunks_from_paths(val_pdf_paths, XML_TRAIN_DIR)\n        if df_val_chunks.empty:\n            print(\"‚ö†Ô∏è Tidak ada chunk yang diekstrak dari set validasi. Lewati fold.\")\n            continue\n        \n        # --- Langkah 4: Muat model LLM ---\n        print(f\"\\nMemuat model {MODEL_NAME}...\")\n        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n        model = AutoModelForCausalLM.from_pretrained(\n            MODEL_NAME,\n            device_map=\"auto\",\n            torch_dtype=torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16,\n        )\n        model.eval()\n        print(\"‚úÖ Model berhasil dimuat.\")\n        \n        # --- Langkah 5: Klasifikasi dan evaluasi ---\n        pred_val_labels = run_llm_classification(df_val_chunks, model, tokenizer)\n        f1 = calculate_f1_score(true_val_labels, pred_val_labels)\n        f1_scores.append(f1)\n    \n    print(f\"\\nRata-rata F1-Score dari {N_FOLDS} folds: {sum(f1_scores) / len(f1_scores):.4f}\")\n    \n    # --- Langkah 6: Proses test set untuk submission ---\n    print(\"\\nMemproses test set untuk submission...\")\n    test_pdf_paths = list(PDF_TEST_DIR.glob(\"*.pdf\"))\n    df_test_chunks = extract_chunks_from_paths(test_pdf_paths, XML_TEST_DIR)\n    if not df_test_chunks.empty:\n        pred_test_labels = run_llm_classification(df_test_chunks, model, tokenizer)\n        if not pred_test_labels.empty:\n            submission = pred_test_labels[['article_id', 'dataset_id', 'type']].copy()\n            submission = submission.sort_values(by=[\"article_id\", \"dataset_id\", \"type\"], ascending=False).drop_duplicates(subset=['article_id', 'dataset_id'], keep=\"first\")\n            submission.insert(0, 'row_id', range(len(submission)))\n            submission.to_csv('submission.csv', index=False)\n            print(\"‚úÖ File submission.csv telah dibuat.\")\n            print(\"Distribusi tipe:\", submission['type'].value_counts())\n        else:\n            print(\"‚ö†Ô∏è Tidak ada prediksi valid untuk test set.\")\n    else:\n        print(\"‚ö†Ô∏è Tidak ada chunk yang diekstrak dari test set.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T02:29:16.503410Z","iopub.execute_input":"2025-07-07T02:29:16.503647Z"}},"outputs":[{"name":"stdout","text":"Membaca data pelatihan...\n\nMelakukan 5-fold cross-validation...\n\nFold 1/5\n‚úÖ Data dibagi: 418 untuk latihan, 105 untuk validasi.\nMemulai ekstraksi dari 105 file...\n","output_type":"stream"},{"name":"stderr","text":"üìÑ Mengekstrak File: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 105/105 [00:06<00:00, 15.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nMemuat model Qwen/Qwen2-7B-Instruct...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa2444c7c159434e9bdb9ca655b58ede"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bea79deba33f4332aeb9359de67f3800"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"080b1a62ac9b47699ccc01e48d7eb820"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6594d3248db4e60ba18976a0919b31c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea37667550c2435d97eb4528ca307a90"}},"metadata":{}},{"name":"stderr","text":"2025-07-07 02:29:40.108824: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751855380.321205      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751855380.383868      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5a1d6684d9e4789999333fbe8ec3bed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26b1b2f31a284aa08003df5354759e01"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40f07e63be734f58975559b7d24bdf2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f55007cdd4d4f589a867f16069b4770"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb87b64a219948d2af1028d715b70b4c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/3.56G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"967b868b1dfe4c4f843885a35032e2d5"}},"metadata":{}},{"name":"stderr","text":"Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d74389ad60d941f4acf0f8e12ca65b07"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a40b6f20ad02489d88a51c50bc6f9f44"}},"metadata":{}},{"name":"stdout","text":"‚úÖ Model berhasil dimuat.\n\nMemulai klasifikasi dengan LLM untuk 26 chunk...\n","output_type":"stream"},{"name":"stderr","text":"ü§ñ Mengklasifikasi:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 3/7 [01:58<02:39, 39.91s/it]","output_type":"stream"}],"execution_count":null}]}