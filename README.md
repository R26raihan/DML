# Description 

In this project, we addressed the task of identifying and classifying dataset citations in scientific articles as either Primary or Secondary. We began by loading the training labels and removing entries with missing dataset references. Then, we filtered the dataset to include only those articles with available XML files, from which we extracted the full article text by parsing <p> tags using Python’s XML utilities. After cleaning the data and ensuring proper text extraction, we performed exploratory data analysis to understand text lengths and label distribution.

We discovered a class imbalance, with Secondary citations significantly outnumbering Primary ones. To resolve this, we oversampled the underrepresented Primary class, creating a balanced dataset. A train-validation split was then performed to enable local model evaluation. Our next step involves training a transformer-based NLP model—such as SciBERT—fine-tuned on the labeled text data, to classify citation types based on the full article context. This setup prepares us to effectively tackle the citation classification challenge using state-of-the-art language modeling techniques.

Summary of the Modeling Workflow
In this project, we developed a text classification model to identify whether a scientific article references a dataset as a Primary or Secondary citation. We began by exploring the labeled training data, where we found that the class distribution was imbalanced, with significantly more Secondary citations. To address this, we applied oversampling on the Primary class to create a balanced dataset. The text data from XML files was extracted and cleaned by lowercasing and removing punctuation. Then, we used TF-IDF vectorization with unigram and bigram features to convert the cleaned text into numerical form suitable for modeling.

After transforming the data, we trained and evaluated multiple machine learning models including Logistic Regression, Naive Bayes, Linear SVC, Random Forest, and XGBoost. The best-performing models achieved F1-scores above 0.93 on the validation set. Finally, we applied the trained model to the official unlabeled test set provided by the competition organizers, processed the XML files, and predicted the citation types for each article. The predictions were saved and are ready for submission or further analysis.
