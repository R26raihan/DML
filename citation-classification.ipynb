{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":82370,"databundleVersionId":12656064,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport re\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport fitz  # PyMuPDF\n\n\npdf_dir = \"/kaggle/input/make-data-count-finding-data-references/train/PDF\"\npdf_paths = list(Path(pdf_dir).glob(\"*.pdf\"))\n\n\nRE_DOI = re.compile(r\"10\\.\\d{4,9}/[\\w.()/:;-]+\", re.I)\nRE_ACCESSION = re.compile(r\"GSE\\d+|SR[APRX]\\d+|PRJ[NAED]\\d+|EPI(?:_ISL_)?\\d+|PXD\\d{6}|SAM[ND]\\d+|ERR\\d+\", re.I)\n\n# Fungsi untuk membersihkan teks referensi\ndef remove_references(text: str) -> str:\n    lines = text.split(\"\\n\")\n    for i in range(len(lines)-1, max(0, int(len(lines)*0.3)), -1):\n        if re.match(r\"^(REFERENCES|BIBLIOGRAPHY|Literature Cited|Works Cited)\", lines[i], re.I):\n            return \"\\n\".join(lines[:i])\n    return text\n\n# Ekstraksi teks & potong di sekitar kandidat referensi\nspan = 200  # jumlah karakter sebelum/sesudah entitas\nchunks = []\n\nfor pdf_path in tqdm(pdf_paths):\n    article_id = pdf_path.stem\n    doc = fitz.open(pdf_path)\n    text = \"\\n\".join([page.get_text() for page in doc])\n    doc.close()\n\n    text = remove_references(text)\n\n    # Ambil potongan teks sekitar DOI\n    for match in RE_DOI.finditer(text):\n        chunk = text[max(0, match.start() - span): match.end() + span]\n        chunks.append({\"article_id\": article_id, \"text\": chunk, \"dataset_id\": match.group(), \"source\": \"doi\"})\n\n    # Ambil potongan teks sekitar Accession ID\n    for match in RE_ACCESSION.finditer(text):\n        chunk = text[max(0, match.start() - span): match.end() + span]\n        chunks.append({\"article_id\": article_id, \"text\": chunk, \"dataset_id\": match.group(), \"source\": \"accession\"})\n\n# Simpan hasil sementara untuk dipakai oleh LLM nanti\nimport pandas as pd\n\ndf_chunks = pd.DataFrame(chunks)\ndf_chunks.to_csv(\"chunks_for_llm.csv\", index=False)\n\nprint(df_chunks.head())\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-05T13:14:56.497447Z","iopub.execute_input":"2025-07-05T13:14:56.498159Z","iopub.status.idle":"2025-07-05T13:15:50.068717Z","shell.execute_reply.started":"2025-07-05T13:14:56.498133Z","shell.execute_reply":"2025-07-05T13:15:50.068143Z"}},"outputs":[{"name":"stderr","text":" 13%|â–ˆâ–Ž        | 66/524 [00:05<00:46,  9.84it/s]","output_type":"stream"},{"name":"stdout","text":"MuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 524/524 [00:53<00:00,  9.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"                     article_id  \\\n0  10.1371_journal.pone.0191086   \n1  10.1371_journal.pone.0191086   \n2  10.1371_journal.pone.0191086   \n3  10.1371_journal.pone.0191086   \n4  10.1371_journal.pone.0191086   \n\n                                                text  \\\n0  sh and other aquatic organisms, particularly f...   \n1  inlola MA, Reygondeau G, Wabnitz\\nCCC, Troell ...   \n2   Global trends in food fish production from 19...   \n3  reshwater\\naquaculture and mariculture. Data f...   \n4   1 Echinodermata) (S1 Table). Although sea-\\nw...   \n\n                          dataset_id source  \n0       10.1371/journal.pone.0191086    doi  \n1                   10.1371/journal.    doi  \n2  10.1371/journal.pone.0191086.g001    doi  \n3       10.1371/journal.pone.0191086    doi  \n4       10.1371/journal.pone.0191086    doi  \n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"from openai import OpenAI\nimport pandas as pd\nimport time\nfrom tqdm import tqdm\n\n# ðŸ”‘ Ganti dengan API KEY kamu\nclient = OpenAI(\n    base_url=\"https://openrouter.ai/api/v1\",\n    api_key=\"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\", \n)\n\nHEADERS = {\n    \"HTTP-Referer\": \"\", \n    \"X-Title\": \"\",  \n}\n\nMODEL_NAME = \"mistralai/mixtral-8x7b-instruct\"\n\n\nSYSTEM_PROMPT = \"\"\"You are a scientific assistant.\nGiven a DOI and a relevant academic text snippet, classify the dataset associated with the DOI as:\nA) Primary â€” generated by this study\nB) Secondary â€” reused from other studies\nC) None â€” not dataset-related or just cited\n\nRespond with exactly one letter: A, B, or C.\n\"\"\"\n\ndef create_prompt(text, dataset_id):\n    return f\"DOI: {dataset_id}\\n\\nText:\\n{text}\"\n\ndef classify_with_llm(text, dataset_id, retries=3):\n    for _ in range(retries):\n        try:\n            messages = [\n                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n                {\"role\": \"user\", \"content\": create_prompt(text, dataset_id)}\n            ]\n\n            completion = client.chat.completions.create(\n                model=MODEL_NAME,\n                messages=messages,\n                extra_headers=HEADERS,\n                extra_body={},\n                temperature=0.1,\n                max_tokens=1\n            )\n\n            return completion.choices[0].message.content.strip()\n        except Exception as e:\n            print(\"Error:\", e)\n            time.sleep(5)\n    return \"C\"\n\n# ðŸ”„ Load data\ndf_chunks = pd.read_csv(\"chunks_for_llm.csv\")\n\n# ðŸ” Proses dalam batch\nBATCH_SIZE = 15  # < 20 per menit\nresults = []\n\nfor i in tqdm(range(0, len(df_chunks), BATCH_SIZE)):\n    batch = df_chunks.iloc[i:i+BATCH_SIZE]\n    \n    for _, row in batch.iterrows():\n        text = row['text']\n        dataset_id = row['dataset_id']\n        article_id = row['article_id']\n\n        result = classify_with_llm(text, dataset_id)\n\n        if result == \"A\":\n            label = \"Primary\"\n        elif result == \"B\":\n            label = \"Secondary\"\n        else:\n            continue  # Skip \"None\"\n\n        results.append({\n            \"article_id\": article_id,\n            \"dataset_id\": dataset_id,\n            \"type\": label\n        })\n    \n    time.sleep(60)  \n\n# ðŸ’¾ Simpan hasil\ndf_results = pd.DataFrame(results)\ndf_results.to_csv(\"llm_predictions.csv\", index=False)\nprint(df_results.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T13:26:49.116653Z","iopub.execute_input":"2025-07-05T13:26:49.117117Z"}},"outputs":[{"name":"stderr","text":"  1%|â–         | 4/278 [04:39<5:22:14, 70.57s/it]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"submission = pd.DataFrame(results).drop_duplicates()\nsubmission['row_id'] = range(len(submission))\nsubmission = submission[[\"row_id\", \"article_id\", \"dataset_id\", \"type\"]]\nsubmission.to_csv(\"submission.csv\", index=False)\n\nprint(\"âœ… Saved submission.csv\")\nprint(submission.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T13:20:22.237736Z","iopub.status.idle":"2025-07-05T13:20:22.238170Z","shell.execute_reply.started":"2025-07-05T13:20:22.238000Z","shell.execute_reply":"2025-07-05T13:20:22.238018Z"}},"outputs":[],"execution_count":null}]}